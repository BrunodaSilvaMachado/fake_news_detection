# ğŸ“° DetecÃ§Ã£o de Fake News com Redes Neurais de Grafos (GNNs)

Este projeto implementa um modelo de **Graph Convolutional Network (GCN)** com **Batch Normalization (GCN_BN)** para a tarefa de **detecÃ§Ã£o de fake news**.  
O trabalho foi desenvolvido como parte de um projeto acadÃªmico e tem como objetivo avaliar a eficÃ¡cia das **redes neurais de grafos** aplicadas ao **processamento de linguagem natural (PLN)**.

---

## ğŸ¯ MotivaÃ§Ã£o
A propagaÃ§Ã£o de **fake news** tem impacto crescente em Ã¡reas sociais, polÃ­ticas e econÃ´micas. Modelos tradicionais de classificaÃ§Ã£o de texto (baseados apenas em embeddings e redes densas) nem sempre capturam as **relaÃ§Ãµes estruturais entre documentos**.  
Neste projeto, exploramos o uso de **grafos de similaridade** construÃ­dos a partir de embeddings GloVe e conectados via **K-Nearest Neighbors (KNN)**, treinando um modelo GCN para classificar notÃ­cias como verdadeiras ou falsas.

---

## ğŸ“Š Dataset
- Fonte: [Mendeley Fake News Dataset](https://data.mendeley.com/datasets/945z9xkc8d/1)  
- PrÃ©-processado e dividido em:
  - **80% treino**, **10% validaÃ§Ã£o**, **10% teste**
- Estrutura:
```csv
  text,label
  "Trump promises new deal for black America...",0
  "Foundation ties bedevil Hillary Clinton...",1
  ...
```
* **Label**:

  * `0` â†’ NotÃ­cia verdadeira
  * `1` â†’ Fake News

---

## ğŸ§© Metodologia

1. **PrÃ©-processamento**: tokenizaÃ§Ã£o, remoÃ§Ã£o de stopwords e padronizaÃ§Ã£o.
2. **Embeddings**: GloVe (100d), cada documento representado pela mÃ©dia dos vetores de suas palavras.
3. **ConstruÃ§Ã£o do grafo**:

   * Similaridade de cosseno entre embeddings de documentos.
   * ConexÃ£o via **KNN** com valores de `k = 3, 5, 8, 16`.
4. **Modelo**:

   * Arquitetura **GCN\_BN**:

     * Camada GCN â†’ BatchNorm â†’ ReLU â†’ Dropout
     * Camada GCN â†’ Softmax (classificaÃ§Ã£o)
   * Treinado por atÃ© 240 Ã©pocas com `lr=0.005` e `dropout=0.5`.
5. **AvaliaÃ§Ã£o**:

   * ValidaÃ§Ã£o cruzada (5-fold).
   * MÃ©tricas: **AcurÃ¡cia, F1-score, PrecisÃ£o, Recall, AUC**.

---

## ğŸ“ˆ Resultados

* Melhor configuraÃ§Ã£o: **k=3**
* AcurÃ¡cia mÃ©dia â‰ˆ **84%**
* F1-score â‰ˆ **84%**
* Resultados consistentes e comparÃ¡veis aos benchmarks da literatura.

Exemplo de comparaÃ§Ã£o ROC (ilustrativa):
![Curvas ROC](data/outputs/roc_curves_sinteticas.png)

---

## ğŸ“‚ Estrutura do RepositÃ³rio

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # Dataset original
â”‚   â”œâ”€â”€ processed/     # Embeddings e grafos salvos
â”‚   â””â”€â”€ outputs/       # Resultados, grÃ¡ficos e mÃ©tricas
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py  # PrÃ©-processamento e embeddings
â”‚   â”œâ”€â”€ graph.py       # ConstruÃ§Ã£o do grafo KNN
â”‚   â”œâ”€â”€ model.py       # DefiniÃ§Ã£o do GCN_BN
â”‚   â””â”€â”€ train.py       # Loop de treino/validaÃ§Ã£o
â”œâ”€â”€ notebooks/         # Notebooks de experimentaÃ§Ã£o
â”œâ”€â”€ requirements.txt   # DependÃªncias do projeto
â””â”€â”€ README.md          # Este arquivo
```

---

## âš™ï¸ Como Executar

1. Clone o repositÃ³rio:

   ```bash
   git clone https://github.com/BrunodaSilvaMachado/fake_news_detection.git
   cd fake_news_detection
   ```
2. Crie o ambiente e instale dependÃªncias:

   ```bash
   pip install -r requirements.txt
   ```
3. Baixe os embeddings do [GloVe 100d](https://nlp.stanford.edu/projects/glove/):

   ```bash
   glove.6B.100d.txt
   ```
4. Rode o treino:

   ```bash
   python src/train.py
   ```

---

## ğŸ‘¨â€ğŸ’» ContribuiÃ§Ãµes

* ImplementaÃ§Ã£o: construÃ§Ã£o de grafos de similaridade e treinamento de GCN.
* Experimentos: anÃ¡lise com diferentes valores de k.
* AvaliaÃ§Ã£o: validaÃ§Ã£o cruzada e comparaÃ§Ã£o com benchmarks da literatura.

---

## ğŸ“š ReferÃªncias

* Kipf, T., & Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks.
* Zhang et al. (2023). Text Classification Based on the Heterogeneous Graph Considering the Relationships between Documents.
* Meel, P., & Vishwakarma, D. (2021). Fake News Detection using Graph Neural Networks.
* Priyanga et al. (2021). Exploring fake news identification using word and sentence embeddings.

