{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "166203e6-0962-4b7f-89c4-74a92c4111d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "166203e6-0962-4b7f-89c4-74a92c4111d4",
        "outputId": "d3e87ed9-446b-439d-f389-ccf66e591e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pypot\n",
            "  Downloading pypot-5.0.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Collecting numpy (from torch_geometric)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Collecting pyserial>2.6 (from pypot)\n",
            "  Downloading pyserial-3.5-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.12/dist-packages (from pypot) (6.4.2)\n",
            "Collecting ikpy==3.0.1 (from pypot)\n",
            "  Downloading ikpy-3.0.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting bottle (from pypot)\n",
            "  Downloading bottle-0.13.4-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from pypot) (4.12.0.88)\n",
            "Collecting wget (from pypot)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from ikpy==3.0.1->pypot) (1.13.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from pypot)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->ikpy==3.0.1->pypot) (1.3.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypot-5.0.2-py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.6/441.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ikpy-3.0.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bottle-0.13.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: annoy, wget\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp312-cp312-linux_x86_64.whl size=551809 sha256=af33596267e5d275c96e88bfc775ed9e64c1e7b0725ad4fb618a09b34af37180\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b9/53/a3b2d1fe1743abadddec6aa541294b24fdbc39d7800bc57311\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=1244e0415d4a225dba75ebd01626efaefa1acad99486f5d568fa61141654095f\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built annoy wget\n",
            "Installing collected packages: wget, pyserial, bottle, annoy, numpy, scipy, opencv-contrib-python, torch_geometric, ikpy, gensim, pypot\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annoy-1.17.3 bottle-0.13.4 gensim-4.3.3 ikpy-3.0.1 numpy-1.26.4 opencv-contrib-python-4.11.0.86 pypot-5.0.2 pyserial-3.5 scipy-1.13.1 torch_geometric-2.6.1 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e262e968e96d4571b39d02f8f2f04738"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install torch_geometric gensim pypot annoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7ec78128-ab32-4e27-a5df-5ee141753650",
      "metadata": {
        "id": "7ec78128-ab32-4e27-a5df-5ee141753650"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx, coalesce, degree, to_undirected, dropout_adj\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1kAF2cv5Z2H",
        "outputId": "c94df952-8344-4c9b-ecb7-52abc12c53a6"
      },
      "id": "Y1kAF2cv5Z2H",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drivestorage = \"/content/drive/MyDrive/Colab Notebooks/data/\"\n",
        "def getDriveStorage(dir):\n",
        "  return drivestorage + dir"
      ],
      "metadata": {
        "id": "u5YcZh2a5b-l"
      },
      "id": "u5YcZh2a5b-l",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7a1c83-b0a0-4969-8c80-9daca6293daa",
      "metadata": {
        "id": "fb7a1c83-b0a0-4969-8c80-9daca6293daa"
      },
      "outputs": [],
      "source": [
        "# Carrega o arquivo CSV em um DataFrame\n",
        "#A proporção é 80% treino 20 teste.\n",
        "df_test = pd.read_csv(\"data/raw/fnd/test.csv\",sep=\";\")\n",
        "df_train = pd.read_csv(\"data/raw/fnd/train.csv\",sep=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a710785-5590-4b4d-98e5-bdb994161668",
      "metadata": {
        "id": "2a710785-5590-4b4d-98e5-bdb994161668",
        "outputId": "46460e4c-00c0-40b2-8824-f97fc267d63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  trspnc park bucs win lose four playoff dang re...      0\n",
            "1  whatever happen type meanwhile m rubin gestate...      1\n",
            "2  review departure watch flirt death photo sad t...      1\n",
            "3  ellen degeneres say never host president trump...      0\n",
            "4  yemen war turn child young bride soldier accor...      1\n",
            "                                                text  label\n",
            "0  trump tell million american hurricane victim r...      0\n",
            "1  new york cable provider altice acc network new...      0\n",
            "2  japan dentsu get small fine overtime breach de...      1\n",
            "3  little kid toxic stress solve priscilla chan c...      1\n",
            "4  fantasy football kicker ranking week warn some...      0\n"
          ]
        }
      ],
      "source": [
        "# Exibe as primeiras linhas do DataFrame\n",
        "print(df_test.head())\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f80544-90a1-40e8-8d89-b3ccbb89d817",
      "metadata": {
        "id": "d7f80544-90a1-40e8-8d89-b3ccbb89d817"
      },
      "source": [
        "# Pre-Processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb0bf07-f285-493d-afbc-18c93d381e1a",
      "metadata": {
        "id": "3eb0bf07-f285-493d-afbc-18c93d381e1a",
        "outputId": "7b4ea8b5-a839-4c6f-e0c2-261eb1d8e4ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text     0\n",
            "label    0\n",
            "dtype: int64 label\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64 287.7889221556886\n"
          ]
        }
      ],
      "source": [
        "print(df_train.isnull().sum(),\n",
        "df_train['label'].value_counts(normalize=True),\n",
        "df_train['text'].str.split().apply(len).mean()\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407a5c30-7697-4180-9768-de8b5d4b911f",
      "metadata": {
        "id": "407a5c30-7697-4180-9768-de8b5d4b911f"
      },
      "outputs": [],
      "source": [
        "df_train['text'].str.lower()\n",
        "df_test['text'].str.lower()\n",
        "#strip accents\n",
        "df_train['text'] = df_train['text'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "df_test['text'] = df_test['text'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29abb397-7181-4146-b00d-adaee3473547",
      "metadata": {
        "id": "29abb397-7181-4146-b00d-adaee3473547"
      },
      "outputs": [],
      "source": [
        "df_train.to_parquet(getDriveStorage(\"interim/train_fnd_clean.parquet\"))\n",
        "df_test.to_parquet(getDriveStorage(\"interim/test_fnd_clean.parquet\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9fe102f2-59cb-4607-adc5-956add8aecda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe102f2-59cb-4607-adc5-956add8aecda",
        "outputId": "fa2135a9-e150-49e7-8697-8a8a42816f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  trspnc park bucs win lose four playoff dang re...      0\n",
            "1  whatever happen type meanwhile m rubin gestate...      1\n",
            "2  review departure watch flirt death photo sad t...      1\n",
            "3  ellen degeneres say never host president trump...      0\n",
            "4  yemen war turn child young bride soldier accor...      1\n",
            "                                                text  label\n",
            "0  trump tell million american hurricane victim r...      0\n",
            "1  new york cable provider altice acc network new...      0\n",
            "2  japan dentsu get small fine overtime breach de...      1\n",
            "3  little kid toxic stress solve priscilla chan c...      1\n",
            "4  fantasy football kicker ranking week warn some...      0\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_parquet(getDriveStorage(\"interim/train_fnd_clean.parquet\"))\n",
        "df_test = pd.read_parquet(getDriveStorage(\"interim/test_fnd_clean.parquet\"))\n",
        "\n",
        "# Exibe as primeiras linhas do DataFrame\n",
        "print(df_test.head())\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "07b80212-be3f-4850-82ad-42989ab6dabf",
      "metadata": {
        "id": "07b80212-be3f-4850-82ad-42989ab6dabf"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    # tudo minúsculo, só letras/números\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text, flags=re.UNICODE).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS and len(t) > 2]\n",
        "    return tokens\n",
        "\n",
        "def document_vector(text, embeddings_index, dim=100):\n",
        "    tokens = preprocess(text)\n",
        "    vecs = [embeddings_index[word] for word in tokens if word in embeddings_index]\n",
        "    if len(vecs) == 0:\n",
        "        return np.zeros(dim)  # fallback para documentos sem palavras conhecidas\n",
        "    return np.mean(vecs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cf6d9f7d-c81b-4be0-9278-05331c9d3b64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6d9f7d-c81b-4be0-9278-05331c9d3b64",
        "outputId": "ae72ba87-ce82-4787-c0d0-f571cd54bd3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palavras no vocabulário GloVe: 400000\n"
          ]
        }
      ],
      "source": [
        "# Caminho do arquivo GloVe baixado e descompactado\n",
        "glove_path = getDriveStorage(\"glove.6B.100d.txt\")\n",
        "\n",
        "# Carregar embeddings em memória\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embeddings_index[word] = vector\n",
        "print(\"Total de palavras no vocabulário GloVe:\", len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2f19b6-9fba-48ce-92c8-b1ec94474561",
      "metadata": {
        "id": "6c2f19b6-9fba-48ce-92c8-b1ec94474561",
        "outputId": "570bb048-b7dc-45bc-f3ae-9959e21bd580"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|                                                                                         | 0/3340 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|▍                                                                              | 19/3340 [00:00<00:17, 188.61it/s]\u001b[A\n",
            "  2%|█▋                                                                             | 69/3340 [00:00<00:08, 367.04it/s]\u001b[A\n",
            "  3%|██▌                                                                           | 111/3340 [00:00<00:08, 390.85it/s]\u001b[A\n",
            "  6%|████▉                                                                         | 210/3340 [00:00<00:05, 625.59it/s]\u001b[A\n",
            " 12%|████████▉                                                                    | 389/3340 [00:00<00:02, 1037.02it/s]\u001b[A\n",
            " 17%|█████████████▎                                                               | 575/3340 [00:00<00:02, 1313.75it/s]\u001b[A\n",
            " 23%|█████████████████▊                                                           | 773/3340 [00:00<00:01, 1525.04it/s]\u001b[A\n",
            " 28%|█████████████████████▋                                                       | 942/3340 [00:00<00:01, 1576.05it/s]\u001b[A\n",
            " 35%|██████████████████████████▌                                                 | 1166/3340 [00:00<00:01, 1777.87it/s]\u001b[A\n",
            " 40%|██████████████████████████████▌                                             | 1344/3340 [00:01<00:01, 1707.84it/s]\u001b[A\n",
            " 47%|███████████████████████████████████▍                                        | 1559/3340 [00:01<00:00, 1834.62it/s]\u001b[A\n",
            " 53%|████████████████████████████████████████▏                                   | 1765/3340 [00:01<00:00, 1896.41it/s]\u001b[A\n",
            " 59%|████████████████████████████████████████████▌                               | 1956/3340 [00:01<00:00, 1879.29it/s]\u001b[A\n",
            " 64%|████████████████████████████████████████████████▉                           | 2148/3340 [00:01<00:00, 1887.81it/s]\u001b[A\n",
            " 70%|█████████████████████████████████████████████████████▏                      | 2338/3340 [00:01<00:00, 1824.96it/s]\u001b[A\n",
            " 77%|██████████████████████████████████████████████████████████▎                 | 2560/3340 [00:01<00:00, 1936.08it/s]\u001b[A\n",
            " 83%|██████████████████████████████████████████████████████████████▊             | 2758/3340 [00:01<00:00, 1945.00it/s]\u001b[A\n",
            " 89%|███████████████████████████████████████████████████████████████████▍        | 2964/3340 [00:01<00:00, 1973.53it/s]\u001b[A\n",
            "100%|████████████████████████████████████████████████████████████████████████████| 3340/3340 [00:02<00:00, 1612.12it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape final: (3340, 100) (3340,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Gerar embeddings\n",
        "X_train = np.vstack([document_vector(text, embeddings_index, 100)\n",
        "                     for text in tqdm(df_train[\"text\"])])\n",
        "\n",
        "y_train = df_train[\"label\"].values\n",
        "\n",
        "print(\"Shape final:\", X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1768205-d398-4a98-bd56-0fe27a38a7af",
      "metadata": {
        "id": "c1768205-d398-4a98-bd56-0fe27a38a7af"
      },
      "outputs": [],
      "source": [
        "# salvar embeddings e labels\n",
        "np.save(\"data/processed/X_train_fnd_glove100.npy\", X_train)\n",
        "np.save(\"data/processed/y_train_fnd.npy\", y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d182c44-0502-4e14-988a-90c26ceddebe",
      "metadata": {
        "id": "1d182c44-0502-4e14-988a-90c26ceddebe"
      },
      "outputs": [],
      "source": [
        "# para o teste também\n",
        "X_test = np.vstack([document_vector(text, embeddings_index, 100)\n",
        "                    for text in df_test[\"text\"]])\n",
        "y_test = df_test[\"label\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919be55a-0114-4c2e-beb0-b5a9b25c6001",
      "metadata": {
        "id": "919be55a-0114-4c2e-beb0-b5a9b25c6001"
      },
      "outputs": [],
      "source": [
        "np.save(\"data/processed/X_test_fnd_glove100.npy\", X_test)\n",
        "np.save(\"data/processed/y_test_fnd.npy\", y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6c8f6a72-d06b-4a78-8305-c67fca5f2417",
      "metadata": {
        "id": "6c8f6a72-d06b-4a78-8305-c67fca5f2417"
      },
      "outputs": [],
      "source": [
        "# reabrir\n",
        "X_train = np.load(getDriveStorage(\"processed/X_train_fnd_glove100.npy\"))\n",
        "y_train = np.load(getDriveStorage(\"processed/y_train_fnd.npy\"))\n",
        "X_test = np.load(getDriveStorage(\"processed/X_test_fnd_glove100.npy\"))\n",
        "y_test = np.load(getDriveStorage(\"processed/y_test_fnd.npy\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33257d6-13d8-4509-8c37-dfc1a3baf15e",
      "metadata": {
        "id": "c33257d6-13d8-4509-8c37-dfc1a3baf15e"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f6924e87-8e29-4249-a5c8-274f3ca87933",
      "metadata": {
        "id": "f6924e87-8e29-4249-a5c8-274f3ca87933"
      },
      "outputs": [],
      "source": [
        "# Carregar GloVe já no formato word2vec (se não fez antes, converter)\n",
        "glove_file = getDriveStorage(\"glove.6B.100d.txt\")\n",
        "tmp_file = getDriveStorage(\"glove.6B.100d.word2vec.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fef6f18-1961-4d78-8e61-e489d691d206",
      "metadata": {
        "id": "8fef6f18-1961-4d78-8e61-e489d691d206"
      },
      "outputs": [],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_file, tmp_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "48b42aa8-a1c4-494f-8db0-84fe0a2b22ae",
      "metadata": {
        "id": "48b42aa8-a1c4-494f-8db0-84fe0a2b22ae"
      },
      "outputs": [],
      "source": [
        "# Agora carregamos como modelo gensim\n",
        "glove_model = KeyedVectors.load_word2vec_format(tmp_file, binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe5f85f-91bc-4768-a9af-42fce72f16f2",
      "metadata": {
        "id": "ebe5f85f-91bc-4768-a9af-42fce72f16f2"
      },
      "source": [
        "### Word Mover’s Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7526226b-e2e9-4571-b64c-d63bbd95f11a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7526226b-e2e9-4571-b64c-d63bbd95f11a",
        "outputId": "6cb45431-d371-444c-bbb2-07dd55007316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized wmdistance function and helper function defined.\n"
          ]
        }
      ],
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "def wmdistance(docs, embeddings, n, k=3, n_neighbors_ann=50):\n",
        "    \"\"\"\n",
        "    Optimized WMD calculation using Approximate Nearest Neighbor (ANN) search.\n",
        "\n",
        "    Args:\n",
        "        docs (list): List of tokenized documents.\n",
        "        embeddings (np.ndarray): Array of document embeddings (e.g., average GloVe embeddings).\n",
        "        n (int): Number of documents to process.\n",
        "        k (int): Number of nearest neighbors to find using WMD.\n",
        "        n_neighbors_ann (int): Number of approximate nearest neighbors to find using ANN.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Edge index in PyG format.\n",
        "    \"\"\"\n",
        "    edges = []\n",
        "    vector_size = embeddings.shape[1]\n",
        "\n",
        "    # Build ANN index\n",
        "    ann_index = AnnoyIndex(vector_size, 'angular')  # Using angular distance for normalized embeddings\n",
        "    for i in range(n):\n",
        "        ann_index.add_item(i, embeddings[i])\n",
        "    ann_index.build(10)  # 10 trees\n",
        "\n",
        "    for i in tqdm(range(n)):\n",
        "        # Find approximate nearest neighbors using ANN\n",
        "        nearest_neighbors_ann = ann_index.get_nns_by_item(i, n_neighbors_ann + 1) # +1 to include the item itself\n",
        "        nearest_neighbors_ann = [nn for nn in nearest_neighbors_ann if nn != i] # Exclude the item itself\n",
        "\n",
        "        dists = []\n",
        "        # Calculate WMD only for approximate nearest neighbors\n",
        "        for j in nearest_neighbors_ann:\n",
        "            try:\n",
        "                d = glove_model.wmdistance(docs[i], docs[j])\n",
        "                dists.append((j, d))\n",
        "            except Exception as e:\n",
        "                # Handle potential errors during WMD calculation (e.g., out-of-vocabulary words)\n",
        "                # print(f\"Error calculating WMD between doc {i} and {j}: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "        # Sort by distance and get top-k\n",
        "        dists = sorted(dists, key=lambda x: x[1])[:k]\n",
        "        for j, d in dists:\n",
        "            edges.append((i, j))\n",
        "\n",
        "    # Bidirectional edges\n",
        "    edges = edges + [(j,i) for (i,j) in edges]\n",
        "    # edge_index in PyG format\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "# To use this optimized function, you would first need document embeddings.\n",
        "# A simple approach is to average the word embeddings for each document.\n",
        "# Example of how to get average embeddings (assuming docs is the list of tokenized documents):\n",
        "def get_avg_embeddings(docs, model, vector_size):\n",
        "    embeddings = []\n",
        "    for doc in docs:\n",
        "        word_embeddings = [model[word] for word in doc if word in model]\n",
        "        if word_embeddings:\n",
        "            embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "        else:\n",
        "            embeddings.append(np.zeros(vector_size)) # Handle empty documents or documents with only OOV words\n",
        "    return np.array(embeddings)\n",
        "\n",
        "print(\"Optimized wmdistance function and helper function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2ddb8363-f149-438e-8677-1a7c68b14fc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ddb8363-f149-438e-8677-1a7c68b14fc5",
        "outputId": "46977a97-c3f9-4cd5-a396-197521497aad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3340"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(df_train[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3"
      ],
      "metadata": {
        "id": "A_Ag8JDb9kut"
      },
      "id": "A_Ag8JDb9kut",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197faf2b-101a-4939-82c1-7345f16dc860",
      "metadata": {
        "id": "197faf2b-101a-4939-82c1-7345f16dc860"
      },
      "outputs": [],
      "source": [
        "# calcular o edge index do treino com wmdistance\n",
        "#O WMD funciona melhor se os textos forem tokenizados.\n",
        "docs = [preprocess(t) for t in df_train[\"text\"].tolist()]\n",
        "edge_index_train = wmdistance(docs,embeddings_index, len(docs), k)\n",
        "print(edge_index_train.shape)  # (2, num_edges)\n",
        "\n",
        "# edge_index é um tensor torch (2, num_edges)\n",
        "np.save(getDriveStorage(f\"processed/edge_index_train_fnd_k{k}.npy\"), edge_index_train.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ccb1d34-585f-40b3-9bcb-f885be8a4a42",
      "metadata": {
        "id": "4ccb1d34-585f-40b3-9bcb-f885be8a4a42"
      },
      "outputs": [],
      "source": [
        "# para test.\n",
        "docs_test = [preprocess(t) for t in df_test[\"text\"].tolist()]\n",
        "edge_index_test = wmdistance(docs_test,embeddings_index, len(docs), 3)\n",
        "np.save(getDriveStorage(f\"processed/edge_index_test_fnd_k{k}.npy\"), edge_index_test.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c0b5ef-63e6-4f4e-b224-533f70d6883b",
      "metadata": {
        "id": "44c0b5ef-63e6-4f4e-b224-533f70d6883b"
      },
      "outputs": [],
      "source": [
        "# carregar os edege index calculados com wmdistance\n",
        "# carregar o edge_index\n",
        "edge_index_train = torch.from_numpy(np.load(getDriveStorage(f\"processed/edge_index_train_fnd_k{k}.npy\")))\n",
        "edge_index_test = torch.from_numpy(np.load(getDriveStorage(f\"processed/edge_index_test_fnd_k{k}.npy\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similaridade de Cosseno"
      ],
      "metadata": {
        "id": "myuOkzgP8l3o"
      },
      "id": "myuOkzgP8l3o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4aa53ac-2da0-4e95-be9b-bdcfac7df688",
      "metadata": {
        "id": "e4aa53ac-2da0-4e95-be9b-bdcfac7df688"
      },
      "outputs": [],
      "source": [
        "def build_graph_cosine_weighted(X, k=3):\n",
        "    sim = cosine_similarity(X)\n",
        "    N = sim.shape[0]\n",
        "    edges, weights = [], []\n",
        "\n",
        "    for i in tqdm(range(N)):\n",
        "        idx = np.argsort(-sim[i])[1:k+1]    # top-k (exclui i)\n",
        "        for j in idx:\n",
        "            edges += [(i, j), (j, i)]\n",
        "            w = sim[i, j]\n",
        "            weights += [w, w]\n",
        "\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
        "    # Remove duplicatas e garante alinhamento\n",
        "    edge_index, edge_weight = coalesce(edges, weights, num_nodes=N, reduce=\"mean\")\n",
        "    return edge_index, edge_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e92f156-4a5a-4956-8269-bb9fce6d5c4f",
      "metadata": {
        "id": "5e92f156-4a5a-4956-8269-bb9fce6d5c4f"
      },
      "outputs": [],
      "source": [
        "X_all = np.vstack([X_train, X_test])\n",
        "k = 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index, edge_weight = build_graph_cosine_weighted(X_all, k=k)\n",
        "\n",
        "np.save(getDriveStorage(f\"processed/edge_index_x_all_w_k{k}.npy\"), edge_index.numpy())\n",
        "np.save(getDriveStorage(f\"processed/edge_weight_x_all_w_k{k}.npy\"), edge_weight.numpy())"
      ],
      "metadata": {
        "id": "h3Hod3R4833b"
      },
      "id": "h3Hod3R4833b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = (pd.concat([df_train, df_test], ignore_index=True))[\"label\"].values\n",
        "np.save(getDriveStorage(\"processed/labels.npy\"), labels)"
      ],
      "metadata": {
        "id": "U-IGbT1M830O"
      },
      "id": "U-IGbT1M830O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.load(getDriveStorage(\"processed/labels.npy\"))"
      ],
      "metadata": {
        "id": "R80RLJW383xf"
      },
      "id": "R80RLJW383xf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grafo de Similaridade"
      ],
      "metadata": {
        "id": "zpZ86KUx-Hc0"
      },
      "id": "zpZ86KUx-Hc0"
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(labels)\n",
        "train_size = len(df_train)\n",
        "test_size = len(df_test)\n",
        "\n",
        "train_mask = torch.zeros(N, dtype=torch.bool)\n",
        "test_mask = torch.zeros(N, dtype=torch.bool)\n",
        "val_mask = torch.zeros(N, dtype=torch.bool)\n",
        "\n",
        "# Usa 10% do treino como validação\n",
        "val_size = int(train_size * 0.1)\n",
        "\n",
        "train_mask[:train_size-val_size] = True\n",
        "val_mask[train_size-val_size:train_size] = True\n",
        "test_mask[train_size:] = True"
      ],
      "metadata": {
        "id": "MOa3oYyf83um"
      },
      "id": "MOa3oYyf83um",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que você já tem:\n",
        "# X_all: np.array (N, 100) com embeddings de todos os documentos\n",
        "# labels: np.array (N,) com rótulos 0/1\n",
        "# edge_index: tensor (2, num_edges) com arestas\n",
        "# train_mask, val_mask, test_mask: tensores booleanos\n",
        "\n",
        "# Converter para tensores\n",
        "scaler = StandardScaler()\n",
        "X_all = scaler.fit_transform(X_all)\n",
        "x = torch.tensor(X_all, dtype=torch.float)\n",
        "y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Adicionar máscaras\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "data.edge_weight = edge_weight\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "yWJJyd8t83rk"
      },
      "id": "yWJJyd8t83rk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCN BN"
      ],
      "metadata": {
        "id": "cyR01yOX-b4C"
      },
      "id": "cyR01yOX-b4C"
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN_BN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L6_uDBZN83o8"
      },
      "id": "L6_uDBZN83o8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GCN_BN(in_channels=data.num_node_features,\n",
        "            hidden_channels=64,\n",
        "            out_channels=data.y.max().item()+1,\n",
        "            dropout=0.5).to(device)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "rhPJrasA83mC"
      },
      "id": "rhPJrasA83mC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics():\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1).cpu().numpy()\n",
        "    true = data.y.cpu().numpy()\n",
        "\n",
        "    results = {}\n",
        "    for name, mask in zip([\"train\", \"val\", \"test\"],\n",
        "                          [data.train_mask, data.val_mask, data.test_mask]):\n",
        "        mask = mask.cpu().numpy()\n",
        "        y_true, y_pred = true[mask], pred[mask]\n",
        "\n",
        "        acc = (y_true == y_pred).mean()\n",
        "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        precision = precision_score(y_true, y_pred, average=\"macro\")\n",
        "        recall = recall_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "        results[name] = {\n",
        "            \"acc\": acc,\n",
        "            \"f1\": f1,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall\n",
        "        }\n",
        "        # Para o conjunto de teste, retornar também os scores\n",
        "        if name == \"test\":\n",
        "            results[name]['scores'] = out[mask].cpu().detach().numpy()\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "QG8asC_m83jG"
      },
      "id": "QG8asC_m83jG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treino"
      ],
      "metadata": {
        "id": "Dby0dcM_-_D7"
      },
      "id": "Dby0dcM_-_D7"
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "labels = data.y.cpu().numpy()\n",
        "idx_all = np.arange(len(labels))\n",
        "\n",
        "all_metrics = []\n",
        "history_metrics = {}\n",
        "y_np = data.y.cpu().numpy()\n",
        "classes = np.unique(y_np)\n",
        "cw = compute_class_weight('balanced', classes=classes, y=y_np)\n",
        "class_weights = torch.tensor(cw, dtype=torch.float, device=device)\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(idx_all)):\n",
        "    start_time = time.time()  # início do treino do fold\n",
        "    # Máscaras\n",
        "    train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
        "    val_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
        "\n",
        "    # separa validação (10% do treino)\n",
        "    val_size = int(0.1 * len(train_idx))\n",
        "    val_idx = train_idx[:val_size]\n",
        "    train_idx = train_idx[val_size:]\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    val_mask[val_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    # Atualiza masks no objeto data\n",
        "    data.train_mask = train_mask.to(device)\n",
        "    data.val_mask = val_mask.to(device)\n",
        "    data.test_mask = test_mask.to(device)\n",
        "\n",
        "    # Update edge_index and edge_weight in the data object for each fold\n",
        "    data.edge_index = edge_index.to(device)\n",
        "    data.edge_weight = edge_weight.to(device)\n",
        "\n",
        "    # Novo modelo por fold\n",
        "    model = GCN_BN(in_channels=data.num_node_features,\n",
        "                hidden_channels=32,\n",
        "                out_channels=data.y.max().item()+1,\n",
        "                dropout=0.5).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Treino\n",
        "    best_val_f1 = 0\n",
        "    best_metrics = None\n",
        "    history_metrics[fold] = []\n",
        "\n",
        "    for epoch in range(1, 300):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metrics = evaluate_metrics()\n",
        "        history_metrics[fold].append(metrics.copy())\n",
        "        if metrics[\"val\"][\"f1\"] > best_val_f1:\n",
        "            best_val_f1 = metrics[\"val\"][\"f1\"]\n",
        "            best_metrics = metrics\n",
        "\n",
        "    metrics = best_metrics[\"test\"]\n",
        "    metrics[\"train_time\"] = time.time() - start_time\n",
        "    all_metrics.append(metrics)\n",
        "    print(f\"Fold {fold+1} | Test F1: {metrics['f1']:.4f} | \"\n",
        "          f\"Acc: {metrics['acc']:.4f} | Tempo: {metrics['train_time']:.2f}s\")\n"
      ],
      "metadata": {
        "id": "17Do8v3983gD"
      },
      "id": "17Do8v3983gD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "z6DxYljT_CiK"
      },
      "id": "z6DxYljT_CiK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Média e desvio padrão das métricas\n",
        "# Excluir 'scores' da lista de métricas para cálculo de média e desvio padrão\n",
        "metrics_to_average = [m for m in all_metrics[0] if m != 'scores']\n",
        "\n",
        "avg_metrics = {m: np.mean([fold[m] for fold in all_metrics]) for m in metrics_to_average}\n",
        "std_metrics = {m: np.std([fold[m] for fold in all_metrics]) for m in metrics_to_average}\n",
        "\n",
        "print(f\"\\nResultados Médios (5-Fold Cross Validation) para k = {k}:\")\n",
        "for metric in avg_metrics:\n",
        "    print(f\"{metric.capitalize()} | Média: {avg_metrics[metric]:.4f} ± {std_metrics[metric]:.4f}\")"
      ],
      "metadata": {
        "id": "KDizTN7v83db"
      },
      "id": "KDizTN7v83db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma lista vazia para armazenar os dados de cada fold\n",
        "all_history_data = []\n",
        "\n",
        "# Iterar sobre cada fold no dicionário history_metrics\n",
        "for fold_id, fold_data in history_metrics.items():\n",
        "    # Para cada dicionário na lista do fold, adicionar o ID do fold\n",
        "    for epoch_data in fold_data:\n",
        "        epoch_data['fold'] = fold_id\n",
        "        all_history_data.append(epoch_data)\n",
        "\n",
        "# Converter a lista de dicionários para um DataFrame\n",
        "df_hm = pd.DataFrame(all_history_data)\n",
        "\n",
        "# Expandir as colunas de dicionário (train, val, test)\n",
        "df_hm = pd.json_normalize(df_hm.to_dict('records'))\n",
        "df_hm.columns = df_hm.columns.str.replace('.', '_', regex=False)\n",
        "\n",
        "#print(df_hm.head())\n",
        "df_hm"
      ],
      "metadata": {
        "id": "wOnuvaXH83a1"
      },
      "id": "wOnuvaXH83a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hm.to_csv(getDriveStorage(f\"outputs/gcnbnk{k}_fnd_crossval_history.csv\"))\n",
        "df_hm.to_excel(getDriveStorage(f\"outputs/gcnbnk{k}_fnd_crossval_history.xlsx\"))"
      ],
      "metadata": {
        "id": "MfDQ5Rwh83Xz"
      },
      "id": "MfDQ5Rwh83Xz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo: salvar o modelo do último fold\n",
        "torch.save(model.state_dict(), getDriveStorage(f\"outputs/gcnbnk{k}_fnd_model_fold5.pth\"))"
      ],
      "metadata": {
        "id": "vkh5phWS83Ut"
      },
      "id": "vkh5phWS83Ut",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para carregar depois:\n",
        "model = GCN_BN(in_channels=data.num_node_features,\n",
        "            hidden_channels=64,\n",
        "            out_channels=data.y.max().item()+1,\n",
        "            dropout=0.5)\n",
        "model.load_state_dict(torch.load(getDriveStorage(f\"outputs/gcnbnk{k}_fnd_model_fold5.pth\")))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "HPvW6UAh83R_"
      },
      "id": "HPvW6UAh83R_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte lista de dicts em DataFrame\n",
        "df_results = pd.DataFrame(all_metrics)\n",
        "\n",
        "# Excluir 'scores' da lista de métricas para cálculo de média e desvio padrão\n",
        "metrics_to_average = [m for m in df_results.columns if m != 'scores']\n",
        "\n",
        "# Calcula médias e desvios\n",
        "mean_metrics = df_results[metrics_to_average].mean().to_dict()\n",
        "std_metrics = df_results[metrics_to_average].std().to_dict()\n",
        "\n",
        "# Adiciona linhas \"Mean\" e \"Std\"\n",
        "df_results.loc[\"Mean\", metrics_to_average] = mean_metrics.values()\n",
        "df_results.loc[\"Std\", metrics_to_average] = std_metrics.values()\n",
        "\n",
        "# Excluir a coluna 'scores' antes de salvar\n",
        "df_results_to_save = df_results.drop(columns=['scores'])\n",
        "\n",
        "# Salva em CSV e Excel\n",
        "df_results_to_save.to_csv(getDriveStorage(f\"outputs/gcnbnk{k}_fnd_crossval_results.csv\"))\n",
        "df_results_to_save.to_excel(getDriveStorage(f\"outputs/gcnbnk{k}_fnd_crossval_results.xlsx\"))\n",
        "\n",
        "print(df_results)"
      ],
      "metadata": {
        "id": "6OO0nR_i83PK"
      },
      "id": "6OO0nR_i83PK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bo6_IUn583MW"
      },
      "id": "bo6_IUn583MW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEQ4SjhD83JZ"
      },
      "id": "sEQ4SjhD83JZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKfyFgU883GT"
      },
      "id": "SKfyFgU883GT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNJGET8n83DX"
      },
      "id": "NNJGET8n83DX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOm1N-9f83As"
      },
      "id": "gOm1N-9f83As",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwVk9QS8829w"
      },
      "id": "FwVk9QS8829w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkOe-Gkq827T"
      },
      "id": "MkOe-Gkq827T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfOBH9yr824T"
      },
      "id": "WfOBH9yr824T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-k5ryIk821h"
      },
      "id": "u-k5ryIk821h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mQBFjiT82yh"
      },
      "id": "8mQBFjiT82yh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrCYS2QU82v-"
      },
      "id": "nrCYS2QU82v-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYq99zHs82tE"
      },
      "id": "xYq99zHs82tE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDu35MVG82qM"
      },
      "id": "aDu35MVG82qM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yZ3TeSM82nr"
      },
      "id": "1yZ3TeSM82nr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYnjV3mi82lh"
      },
      "id": "NYnjV3mi82lh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zfo1Fr7q82iB"
      },
      "id": "Zfo1Fr7q82iB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8IibeO882e5"
      },
      "id": "l8IibeO882e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "urPRcSze82cL"
      },
      "id": "urPRcSze82cL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gereydxj82ZV"
      },
      "id": "gereydxj82ZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ex7_8j6F82WQ"
      },
      "id": "ex7_8j6F82WQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaqQumzi82TR"
      },
      "id": "gaqQumzi82TR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1wMofbGO82QA"
      },
      "id": "1wMofbGO82QA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3la2uVo382NK"
      },
      "id": "3la2uVo382NK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aoRqtmoM82KU"
      },
      "id": "aoRqtmoM82KU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTDH6TTQ82HV"
      },
      "id": "wTDH6TTQ82HV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJKrz2ya82EV"
      },
      "id": "PJKrz2ya82EV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zx6-cygR82Bf"
      },
      "id": "zx6-cygR82Bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dsBuk-381od"
      },
      "id": "4dsBuk-381od",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}